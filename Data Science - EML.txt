SUPERVISED LEARNING
- disebut supervised karena ada jawaban benarnya, ada labelnya dan juga targetnya
- Regresi --> mencari tren dari data yang numerik --> trend finder
- Classification --> sifatnya categorical; pilih A atau B --> pattern finder
- keyword: prediction

UNSUPERVISED LEARNING
- mencari pola dari sesuatu yang tidak ada status nya
- hanya ada data, tidak ada label
- keyword: recommendation 

REINFORCEMENT LEARNING





JUPYTER
opening jupyter via anaconda prompt
- open anaconda prompt
- open folder that is being to be opened
- type >>>jupyter notebook




DOCSTRING
- docstring --> shift + tab
- jika suatu fungsi tidak memunculkan sst ketika di run, maka function itu adalah in place function 





SLICING ON THE LIST PRINCIPLE
- inclusive:exclusive



SEJARAH CONDA
sebelumnya python tidak banyak pakai karena kesulitan orang pakai 
lalu dibuatlah PyPi untuk mengumpulkan library (marketplace)
lalu muncullah Conda yang merapikan library lebih baik dari PyPi (marketplace)
cara install 
- PyPi --> pip install
- Conda --> conda install

kapan pakai pip atau conda? --> selama ada di conda, pakai conda, jika tidak ada baru pakai pypi

cara kerja Conda
1. install Conda (miniconda atau anaconda)
2. default conda adalah membuatkan "base" pada prompt
	di dalamnya sudah ada python 3, conda dan bisa ditambahkan (jupyter, nb_conda_)
	anaconda adalah all in one (sudah meng cover sekitar 700 libraries)
3. dalam conda bisa dibuat berbagai environment dengan berbagai library yang dapat disesuaikan
4. best practice --> buat environment sendiri setiap kali buat project
5. environment ini bisa dibagikan ke sesama team
6. environment ini bersifat global
7. instal dengan anaconda bisa lama karena solving dan checking conflict sangat lama (cek 700an packages)
8. jupyter diinstal di base dan bisa interact dengan environment lain

best pratice di Conda
- pakai miniconda
- base jangan diisi apa2 kecuali jupyter
- install packages dengan batch installation
- check dengan python check_installation.py

big data
definisi mudah: jika pc sudah tidak kuat maka harus pakai tool big data --> java environment

CONDA CODE
1. Check Environment --> conda env list
	Remove Environment --> conda env remove --name nama_env
2. Install package di suatu environment --> conda install --name nama_environment numpy
3. Check list of all libraries --> conda list
4. Check list in the environment --> conda list --name coba_env
5. Connect environment to jupyter --> conda install --name coba_env nb_conda_kernels
6. Other cheatsheets
	pip install <nama_package>
	pip install <nama_package>==<versi>
	pip install -r requirements.txt

	conda install <nama_package>
	conda install <nama_package>==<versi>

	conda create --name <nama_env> <basis>
	conda list
	conda env list

	conda env create -f environment.yml
	conda env remove --name <nama_env>


cek env --> conda env list
hapus env --> conda env remove --name coba_env







PANDAS
PAnel DAtaS

DATAFRAME
column = series


TOPICS FOR WRITING
Article 1
Data Type

Article 2
How to set Python tool







MISSING VALUES
1. ask --> tanya yang beri data
2. drop --> dibuang
3. impute --> diisi


df_cereal.rating.transform(lambda x: x/x.sum())


HANDLING CATEGORICAL DATA
1. ordinal encoding & label encoding
- menomori setiap data category
- kelemahannya adalah tidak fair karena ada label dinomori 0 atau 1 atau 2
- efektif dipakai jika label nya menandakan urutan [binary (gender), ranking description (tidak bagus, bagus, sangat bagus)]
2. One Hot Encoding

Binning
data tertentu dikelompokkan dalam suatu kategori supaya polanya terkelompok dan pada
contohny: kategori balita - anak - remaja - dewasa - lansia (yang masing-masing punya range usia)
contohny: pendapatan masyarakat



Plot
numerik - histogram
kategorik - categorical plot



ALGORITHM

KNN
- lazy learning --> just look at the nearest point

Titanic Data
- prediksi apakah penumpang selamat atau tidak dari kecelakaan titanic

-- note 
	input/feature	target/label
	X		y
-> survived = target; yang lain jadi input

Distance on KNN
- euclidean
- manhattan
- minkowski 


MACHINE LEARNING --> Scikit-Learn
DEEP LEARNING --> Keras, Tensorflow, etc


****
Dataset Split
1) Regression --> dataset diacak dulu baru dipisah

Shuffle Split
2) Classification --> pisahkan dulu sesuai kelasnya, baru masing-masing kelas diajak lalu dipisah

Stratified Shuffle Split
****


Improve data --> tuning dan scaling


IMPORT DATA
REVIEW DATA
HANDLE MISSING DATA 
SCIKIT LEARN (DATA IN X AND y) --> DECIDE FEATURE AND LABEL
HANDLE OVERFIT
	1. data split
	2. make multiple scenarios - kfold validation
	3. if want to improve
	- improve data (scaling)
	- improve model (change or tune model)


Training & Testing Data Analogy
- murid belajar untuk mempersiapkan ujian
- murid diberikan PR (train) oleh guru untuk menghadapi ujian (test)
- murid bisa mempelajari PR untuk mencari berbagai strategi yang paling baik untuk mendapatkan nilai yang baik ketika mengerjakan ujian







functions
- value_counts() --> display sum of category in one column


renew and replace existing package







GUIDELINE
1. Installation
- Jupyter 
- Conda environment

2. Feature Engineering
- binning
	* age binning ===>>> df_titanic.Age = pd.cut(df_titanic.Age, [0, 5, 12, 18, 40, 120], labels=["toddler", "children", "teenager", "adult", "elderly"])
	* fare binning ===>>> df_titanic.Fare = pd.cut(df_titanic.Fare, [0, 25, 100, 600], labels=["cheap", "expensive", "executive"])
	* # ubah target menjadi biner
			df_heart_disease.target = df_heart_disease.target.map(
			       {0: False,
                                1: True,
                                2: True,
                                3: True,
                                4: True})

3. EDA --> Part 4 - EDA dan Feature Engineering - KNN

- encoding


4. Tuning & Scaling
** tuning
cari kombinasi parameter mana yang paling baik, contohnya ketika weight class, cari proporsi bobot/proporsi yang paling okay.

** scaling
knn berhubungan dengan jarak
svm berhubungan margin




Linear Regression
- persamaan garis lurus: y = ax + b
- simulasi online dengan geogebra

koefisien regresi harus balance, jika tidak jangan dipakai ke feature importance


Menyelesaikan Klasifikasi dengan Regresi menggunakan sigmoid
Entropy (prinsip dalam fisika) bisa dipakai untuk klasifikasi
MCE adalah loss dalam klasifikasi. --> Mean Cross Entrophy --> MCE membuat tidak ada plateau, karena plateau akan merusak optimizer gradient descent


Algorithm/Algoritma untuk masing-masing Regresi dan Classification
Reg		Clf
KNN		KNN
SVM		SVM
DT		DT
XGB		XGB
LinReg		LogReg
ElasticNet	NaiveBayes
Ridge


Rules of Thumb
- Positif selalu lebih sedikit
- Mendeteksi yang lebih sedikit, bukan yang banyak.
- PRINSIP
	BALANCE		IMBALANCE (Positif sedikit)
	1. Akurasi		1. F1 - Score
	2. F1 - Score		2. PR Curve
	3. ROC			Jangan pakai ROC dan Akurasi untuk
	4. PR Curve		Imbalance Dataset








RF R-squared Score
2. 0.8343293126316038 0.771613632634213 0.8110924735037599 - complete features trial 2
3. 0.8463976595452549 0.7669263260833341 0.8110986436363258 - SOP n LOR omitted
1. 0.8350934581064868 0.7716428721894208 0.8098228727140044 - complete features





RF R-Squared Score


best score
v2. {'algo__max_depth': 80, 'algo__max_features': 0.5577136220482326, 'algo__min_samples_leaf': 5, 'algo__n_estimators': 122}
0.8752626632301754 0.8014859291745058 0.8231099331088619 
- cv=3, n_iter=50, random


Annisa
algo - elasticnet
categorical: research, numerical: all
scaling: robust


categorical: research, 
cv=3, random




Exam on 29 March 2020

2nd best score
v5. {'algo__n_estimators': 187, 'algo__min_samples_leaf': 6, 'algo__max_features': 0.5, 'algo__max_depth': 65}
0.8659596242653081 0.8009612623374699 0.8265240728123382
- cv=3, n_iter=50, random, 
rfparameter = {'algo__n_estimators': range(120,196),
 'algo__max_depth': range(40, 80),
 'algo__max_features': [0.3, 0.4, 0.5, 0.6, 0.7],
 'algo__min_samples_leaf': range(3, 8)}

3rd best score
v1. {'algo__max_depth': 46, 'algo__max_features': 0.4756699028339012, 'algo__min_samples_leaf': 5, 'algo__n_estimators': 196}
0.8741429363364741 0.7980725181410776 0.816984784266618 
- cv=4, n_iter=50, random

4th best score
v6. {'algo__max_depth': 46, 'algo__max_features': 0.4756699028339012, 'algo__min_samples_leaf': 5, 'algo__n_estimators': 196}
0.8742090314450554 0.8024970885773269 0.8211592739194714


v7. 0.866323471153701 0.802467079663444 0.8193716221986911
v8. {'algo__max_depth': 46, 'algo__max_features': 0.4756699028339012, 'algo__min_samples_leaf': 5, 'algo__n_estimators': 196}
0.8717781600790467 0.8018660935102457 0.8144866387440773

v3. {'algo__max_depth': 20, 'algo__max_features': 0.3, 'algo__min_samples_leaf': 1, 'algo__n_estimators': 200}
0.9720332515007792 0.8011476590541827 0.7785931250623562
- cv3, grid failed

v4. {'algo__max_depth': 26, 'algo__max_features': 0.3607763076223912, 'algo__min_samples_leaf': 2, 'algo__n_estimators': 141, 'prep__numeric__poly__degree': 1, 'prep__numeric__poly__interaction_only': False}
0.9270383910535411 0.8024904611169057 0.7896314305817781
- cv3, random, poly failed



manual picking --> coba randomstate nya.
setelah splitting, plot dulu dengan histogram

cek keterwakilan distribusi data train dan test melalui plotting (residual plot)

pelajari semua parameter dari masing-masing algoritma

Workflow 4 Phases
Phase 1: Think Simple: Use AUTOML as benchmark
Phase 2: Feature Engineering --> before it plot the mean score decrease




